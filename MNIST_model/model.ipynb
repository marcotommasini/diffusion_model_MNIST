{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"U319REkT2gs6"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import sys\n"]},{"cell_type":"code","source":[],"metadata":{"id":"zkYauJediyzo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Predefined blocks\n","\n","#Double convolution with Gelu activation\n","#The size of the input is going to be size of the output, except for the number of channels\n","class d_conv(nn.Module):\n","  def __init__(self, in_channels, out_channels, mid_channels = False, residual = False):    #Residuals define if there is going to be a residual connection from the input to the output\n","    super().__init__()\n","\n","    self.residual = residual\n","\n","    if not mid_channels:\n","      mid_channels = out_channels\n","\n","    self.double_conv = nn.Sequential(\n","      nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n","      nn.GroupNorm(1, mid_channels),\n","      nn.GELU(),\n","      nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","      nn.GroupNorm(1, out_channels),)\n","    \n","  def forward(self, x):\n","    if self.residual:\n","      output = F.gelu(x + self.double_conv(x))\n","    else:\n","      output = self.double_conv(x)\n","    return output\n","\n","#Define the self attention block\n","\n","class SA(nn.Module):    #Standard attention block\n","  def __init__(self, in_channels, dimension_input_image):\n","    super().__init__()\n","    self.in_channels = in_channels\n","    self.dimension_input_image = int(dimension_input_image)\n","    self.mha = nn.MultiheadAttention(in_channels, 4, batch_first=True)\n","    self.layer_norm = nn.LayerNorm([in_channels])\n","\n","    self.feed_forward = nn.Sequential(\n","      nn.LayerNorm([in_channels]),\n","      nn.Linear(in_channels, in_channels),\n","      nn.GELU(),\n","      nn.Linear(in_channels, in_channels),\n","    )\n","  def forward(self, x):\n","    \n","    x = x.view(-1, self.in_channels, int(self.dimension_input_image**2)).swapaxes(1,2)\n","    \n","    norm_x = self.layer_norm(x)\n","    attention_value, _ = self.mha(norm_x, norm_x, norm_x)\n","    attention_value = attention_value + x\n","    attention_value = self.feed_forward(attention_value) + attention_value\n","    output = attention_value.swapaxes(2, 1).view(-1, self.in_channels, self.dimension_input_image, self.dimension_input_image)\n","    return output\n","\n"],"metadata":{"id":"EdZdYVXkGM36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Defining Downsaple, midsample, and upsample blocks\n","\n","class DS(nn.Module):\n","  def __init__(self, in_channels, out_channels, size_time_dimension = 256):\n","    super().__init__()\n","    self.pool_layer = nn.Sequential(   #Pooling layer with some convolutional blocks to change the number of channels and add the resitual\n","        nn.MaxPool2d(2),\n","        d_conv(in_channels, in_channels, residual=True),\n","        d_conv(in_channels, out_channels),\n","    )\n","    self.emb_layer = nn.Sequential(    #It is needed to change the dimensions of the time tensor in order to sum it to the output of the pooling layer\n","        nn.SiLU(),  #Activation function\n","        nn.Linear(size_time_dimension, out_channels)\n","    )\n","  def forward(self, x, time_tensor):\n","    out1 = self.pool_layer(x)\n","    out2 = self.emb_layer(time_tensor)[:, :, None, None].repeat(1,1,out1.shape[-2], out1.shape[-1]) #In this line we first match the number of time dimensions wiht the number of channels\n","                                                                                                    #Then this \"[:, :, None, None].repeat(1,1,x.shape[-2], x.shape[-1])\" part server to match the size of time tensor to the size of the outputx\n","    return (out1 + out2)\n","\n","\n","class US(nn.Module):\n","  def __init__(self, in_channels, out_channels, size_time_dimension = 256):\n","    super().__init__()\n","\n","    self.up_block = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True) #Block to upsample the resolution of the input\n","\n","    self.conv_block = nn.Sequential(    #Convolutional block\n","        d_conv(in_channels, in_channels, residual=True),\n","        d_conv(in_channels, out_channels),\n","    )\n","  \n","    self.emb_layer = nn.Sequential(    #It is needed to change the dimensions of the time tensor in order to sum it to the output of the pooling layer\n","          nn.SiLU(),  #Activation function\n","          nn.Linear(size_time_dimension, out_channels),\n","      )\n","  def forward(self, x, skip_input, time_tensor):\n","    out1 = self.up_block(x)\n","    out2 = torch.cat([skip_input, out1], dim=1)\n","    out3 = self.conv_block(out2)\n","    out4 = self.emb_layer(time_tensor)[:, :, None, None].repeat(1,1,out3.shape[-2], out3.shape[-1]) #In this line we first match the number of time dimensions wiht the number of channels\n","                                                                                                    #Then this \"[:, :, None, None].repeat(1,1,x.shape[-2], x.shape[-1])\" part serves to match the size of time tensor to the size of the outputx\n","    return out3 + out4                                                                                      \n","\n"],"metadata":{"id":"TIHy3Aj0YOdE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class UNET(nn.Module):\n","  def __init__(self, args, in_channels = 1 , out_channels = 1, size_time_dimension = 256, number_classes_input = None, dimension_input_image = 32):\n","    super().__init__()\n","    self.args = args\n","    \n","    self.size_time_dimension = size_time_dimension\n","    self.dimension_input_image = dimension_input_image\n","    #Now we start the definition of the layers with the especific dimensions of the channels\n","    self.in_channel = d_conv(in_channels, 64)\n","    self.down1 = DS(64, 128,self.size_time_dimension)\n","    self.sa1 = SA(128, self.dimension_input_image/2)\n","    self.down2 = DS(128, 256,self.size_time_dimension)\n","    self.sa2 = SA(256, self.dimension_input_image/4)\n","    self.down3 = DS(256, 256,self.size_time_dimension)\n","    self.sa3 = SA(256, self.dimension_input_image/8)\n","\n","    self.bot1 = d_conv(256, 512)\n","    self.bot2 = d_conv(512, 512)\n","    self.bot3 = d_conv(512, 256)\n","\n","    self.up1 = US(512, 128,self.size_time_dimension)\n","    self.sa4 = SA(128, self.dimension_input_image/4)\n","    self.up2 = US(256, 64,self.size_time_dimension)\n","    self.sa5 = SA(64, self.dimension_input_image/2)\n","    self.up3 = US(128, 64,self.size_time_dimension)\n","    self.sa6 = SA(64, self.dimension_input_image)\n","    self.out_channel = nn.Conv2d(64, out_channels, kernel_size=1)\n","\n","    if number_classes_input is not None:\n","      self.classification_embeding = nn.Embedding(number_classes_input, int(size_time_dimension))\n","  \n","  def forward(self, x_input, time_tensor, y_input):\n","\n","        if y_input is not None:\n","          embeded_labels = self.classification_embeding(y_input)\n","          time_tensor = time_tensor + embeded_labels\n","\n","\n","        x1 = self.in_channel(x_input)\n","        x2 = self.down1(x1, time_tensor)\n","        #x2 = self.sa1(x2)\n","        x3 = self.down2(x2, time_tensor)\n","        #x3 = self.sa2(x3)\n","        x4 = self.down3(x3, time_tensor)\n","        #x4 = self.sa3(x4)\n","\n","        x4 = self.bot1(x4)\n","        x4 = self.bot2(x4)\n","        x4 = self.bot3(x4)\n","\n","        x5 = self.up1(x4, x3, time_tensor)\n","        #x5 = self.sa4(x5)\n","        x5 = self.up2(x5, x2, time_tensor)\n","        #x5 = self.sa5(x5)\n","        x5 = self.up3(x5, x1, time_tensor)\n","        #x5 = self.sa6(x5)\n","        output = self.out_channel(x5)\n","        return output\n","\n"],"metadata":{"id":"11GykYzIeK94"},"execution_count":null,"outputs":[]}]}